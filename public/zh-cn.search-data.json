{"/about/":{"data":{"":" 待补充测试 测试 待补充测试 测试 待补充测试 测试 "},"title":"关于"},"/ai/":{"data":{"":"","创建投资组合#创建投资组合":"选择与众不同的新颖项目创建投资组合：\n以 Kaggle 和 阿里天池 等竞赛网站为起点； 将报告在微信公众号、知乎、掘金等平台展示结果； 在 Github 上托管个人博客； 考虑录制一段简短的视频，展示您的发现； ","参考网址#参考网址：":" 应用机器学习获得报酬 2024 年成为数据科学家的学习路径 2024 年学习生成式人工智能的最佳路线图 从数据收集到模型部署：数据科学项目的 6 个阶段 - KDnuggets 全面的 MLOps 学习路径：2024 年版 MLOps 概述 ","学习路径#学习路径":" flowchart LR classDef someclassA fill:#58C9B9 classDef someclassB fill:#9DC8C8 classDef someclassC fill:#f100,stroke-width:1px classDef someclassD fill:#a3c9c7 classDef someclassE fill:#fff,stroke:#fff,color:#fff classDef someclassF fill:#ff9900 A(人工智能):::someclassA A -- 阶段0：基础 --\u003e B(基础知识):::someclassB --\u003e B1(Python\\ R):::someclassC -.-\u003e B2(线性代数\n微积分):::someclassC -.-\u003e B3(概率论\n数理统计\n贝叶斯统计):::someclassC A -- 阶段1：入门 --\u003e C(端到端的\n机器学习):::someclassA --\u003e C12(机器学习概述) --\u003e C13(机器学习算法) --\u003e C14(建模工具\nSklearn) --\u003e C15(建模步骤\nCRISP-DM) --\u003e Y C -.-\u003e C21(自动机器学习\nAuto ML):::someclassC -.-\u003e C22(大数据集处理):::someclassC A -- 阶段2：进阶 --\u003e D(深度学习):::someclassA --\u003e D11(深度学习概述) --\u003e D12(深度学习算法) --\u003e D13(建模工具\nkeras\nPyTorch\nTensorflow\nFastAI) D -.-\u003e D21(深度解析算法):::someclassC -.-\u003e D22(自制框架\nDeZero):::someclassC -.-\u003e D23(工具源码):::someclassC D13 --\u003e D141(自然语言\nNLP) --\u003e Y D13 --\u003e D142(计算机视觉\nCV) --\u003e Y A -- 阶段3：先进 --\u003e F(生成式\n人工智能):::someclassA --\u003e F1(提示工程) F3(从头构建\n生成模型) --\u003e F4(最新趋势\n\\研究\\论文) --\u003e Y F1 --\u003e F21(NLP -\u003e LLM) --\u003e F3 F1 --\u003e F22(CV -\u003e VLM) --\u003e F3 Y(练习\nUCI 数据集\n竞赛\nkaggle\n阿里天池\n...):::someclassA --\u003e Z A -- 阶段4：部署 --\u003e Z1 Z(模型部署\nMLOps):::someclassF Z1(版本控制\\协作:\nGit\\Github) -.-\u003e Z12(操作系统:Linux\n容器化\\云:Docker) -.-\u003e Z13(ML应用平台：\nHF Spaces\\\nStreamlit Sharing) -.-\u003e Z2(《MLOps 概述》\n部署方式\n核心概念\n......) --\u003e Z3(《主要内容》\n自动化管道\n监控\n生命周期管理\n治理) --\u003e Z4(《管理工具》\nMLFlow\nDVC\nPolyaxon\nMetaflow\nKubeflow) Z4 --\u003e Z A -- 补充知识 --\u003e G1(集成学习):::someclassB -.- G2(时间序列):::someclassB -.- G3(迁移学习):::someclassB -.- G4(强化学习):::someclassB -.- G5(专业知识):::someclassB","补充知识#补充知识":"1、集成学习 主要内容参考如下：\n了解集成学习相关概念； 学习集成学习常用算法及集成学习方法体系（Bagging，Boosting，Stacking，Blending，等）； 学习集成学习 Python 库（Scikit-learn，XGBoost，LightGBM，CatBoost）； 练习\\实践。如，小数据集 UCI ML 或 kaggle 等； 通过 Flask API 或 Streamlit\\Gradio 部署应用； 推荐阅读：\n《集成学习：基础与算法》 - 周志华，李楠 2、领域专业知识 作为数据科学家，需要具备解决相关领域的问题，需要理解相关领域的专业知识。 领域专业知识：\n学习不同领域专业知识，如保险，信贷，物流，电商等； 通过研究竞赛平台多领域数据科学问题，获得 多样化的经验 培养 解决问题的技能； 可以通过收集的行业知识\\信息，分析案例，创建行业知识库； ","阶段-0基础知识#阶段 0：基础知识":" 微积分是研究变化\\微分与累积\\积分的数学分支，用于解决运动、曲线、面积等动态问题。 AI 基础 线性代数是研究向量、矩阵和线性变换的数学分支，核心用来解方程、处理空间变换和数据分析。 AI 基础 概率论是研究随机现象规律性的数学分支，用概率量化不确定性。 AI 基础 数理统计是用数学工具（尤其是概率论）从数据中总结规律、预测未知的学科，核心是抽样、估计和假设检验。 AI 基础 Python一种广泛使用的解释型、高级和通用的编程语言。 AI 基础 ","阶段-1端到端的机器学习#阶段 1：端到端的机器学习":" 以学习完整的建模过程为主要目标，以了解常用机器算法（优缺点，原理，步骤，应用）和学习建模工具（Sklearn\\ scikit-learn）为次要目标， 快速熟悉端到端的建模过程。 实践多个案例，熟悉端到端的建模过程，主要内容参考如下：\n了解人工智能，机器学习，深度学习，统计机器学习等相关概念； 学习常用算法原理。了解算法优缺点，原理，步骤，应用即可，不必过多关注数学公式； 学习建模分步过程。如：CRISP-DM； 学习建模工具。如：scikit-learn； 在小数据集上练习。如： the UC Irvine Machine Learning Repository； 将模型打包或序列化后的结果部署为 Flask API 或 Streamlit\\Gradio 应用； 补充内容：\n了解自动化机器学习工具。 了解处理大数据集的 python 库。 推荐阅读：\n《深度学习：从基础到实践》 （上册）- [美] Andrew Glassner ","阶段-2深度学习#阶段 2：深度学习":"深度学习，主要内容参考如下：\n了解深度学习相关概念； 学习深度学习常用算法及深度学习方法体系（CNN，RNN，LSTM，Transformer，等）； 学习深度学习框架\\工具（keras，PyTorch，Tensorflow，FastAI）； 学习自然语言处理，计算机视觉； 在 KAggle，阿里天池上练习； 补充内容：\n机器学习算法深度解析，需要一定数学基础（线性代数，微积分，概率论与数理统计）。 从头开始理解机器学习算法将帮助您为任务选择正确的算法，解释结果，解决高级问题，将算法扩展到新应用程序，并提高现有算法的性能。 深度解析机器学习算法； 学习深度学习自制框架：DeZero； 学习框架\\工具源码； 推荐阅读：\n《深度学习：从基础到实践》 （下册）- [美] Andrew Glassner 《深度学习入门基于Python的理论与实现》 - [日] 斋藤康毅 《深度学习入门2自制框架》 - [日] 斋藤康毅 《深度学习进阶：自然语言处理》 - [日] 斋藤康毅 《深度学习入门4：强化学习》 - [日] 斋藤康毅 《achine Learning Algorithms in Depth》 - VADIM SMOLYAKOV 《统计学习方法》 (第2版) - 李航 《机器学习》（西瓜书）- 周志华 ","阶段-3生成式人工智能#阶段 3：生成式人工智能":"深入研究高级人工智能主题，关注生成模型：\n学习提示工程（专注于创建和改进提示）。如：coze； NLP 的生成模型，LLM（大语言模型）； 计算机视觉的生成模型； 了解如何从头开始构建这些生成模型； 了解生成人工智能的最新趋势和研究； 推荐阅读：\n2024 年学习生成式人工智能的最佳路线图 机器学习的最新进展带代码的论文 10 个学习法学硕士的免费资源 ","阶段-4模型部署#阶段 4：模型部署":"MLOps，机器学习的部署和生命周期管理：\n基础知识：git\\ github\\ Linux\\容器化\\云，HF Spaces\\ Streamlit Sharing； 部署方式：在线部署：批处理，实时（数据库触发器、发布/订阅、Web 服务、应用内）；离线部署（在本地开发环境、测试环境或内部离线环境中部署批处理，实时处理）； 主要内容：自动化管道，监控，生命周期管理，治理； 核心概念：持续集成与持续部署（CI/CD），版本控制，模型监控； 管理工具：MLFlow，Polyaxon，Metaflow，Kubeflow； 推荐阅读：\n成为 MLOps 工程师所需的唯一免费课程：MLOps Zoomcamp 掌握 MLOps 的 10 个 GitHub 存储库 "},"title":"人工智能"},"/ai/end_to_end/":{"data":{"":"\n概述人工智能，机器学习，深度学习，统计机器学习等相关概念； 工具Python 中的机器学习；简单有效的预测数据分析工具； 步骤选择并学习一个过程（CRISP-DM）。学习建模分步过程； 算法了解常用算法原理。了解算法优缺点，原理，应用； 自动机器学习python 自动机器学习的常用库概述；可用来快速选择模型\\算法； 大数据集python 处理大数据集的方法； "},"title":"1. 端到端的机器学习"},"/ai/end_to_end/overview/":{"data":{"":"","1人工智能概述#\u003cfont face=Georgia\u003e1、人工智能概述\u003c/font\u003e":"1.1、相关概念 人工智能\n可以像人类一样学习、理解、思考的计算机程序； 机器学习\n人工智能的一个子集。使用数据训练模型，这些模型可以根据在数据中找到的关系进行预测； 深度学习\n机器学习的一个分支。使用特殊分层结构的机器学习方法（这些分层依次堆叠）；\n深度学习算法，受到大脑结构的启发，可以很好地处理图像、视频或文本等非结构化数据；\n深度学习框架，是一个拼凑了层和函数等组件的库。是一种具有求导功能的编程语言（“可微分编程语言”）；\n自然语言，让计算机理解、分析和生成人类自然语言；\n计算机视觉，让计算机理解和处理图像及视频信息； 生成式人工智能\n可以生成以前从未见过的新内容，如文本，图像，视频等；\n自然语言方向，计算机视觉方向； 1.2、人工智能简史 19 世纪 30~40 年代 —— 分析机（Analytical Engine） 1️⃣ 查尔斯 • 巴贝奇 —— 分析机（Analytical Engine）理念\n查尔斯·巴贝奇从 1834 年开始设计分析机。分析机被认为是现代计算机的先驱，它具有很多现代计算机的特征，如存储程序、通用计算等理念。 2️⃣ 霍华德·艾肯（Howard Aiken）团队 —— 马克一号（Mark I）\n它是 1944 年制造出的一种电动机械计算机，能够进行各种复杂的数学计算，被认为是 第一台通用的机械式计算机。它的用途仅仅是利用机械操作将数学分析领域的某些计算自动化，因此得名“分析机”。 3️⃣ 沃伦·麦卡洛克（Warren McCulloch）,沃尔特·皮茨（Walter Pitts） —— 神经元的数学模型\n1943 年提出了一种形式神经元的数学模型，被认为是 最早的神经网络模型之一。 20 世纪 50 年代 —— 符号主义人工智能（symbolic AI） 符号主义人工智能（symbolic AI）\n精心编写足够多的明确规则来处理知识，就可以实现与人类水平相当的人工智能。\n阿兰 • 图灵 —— “图灵测试”\n人工智能先驱阿兰 • 图灵在其 1950年发表的具有里程碑意义的论文“计算机器和智能”。\n在这篇论文中，图灵提出了著名的“图灵测试”，用于判断机器是否具有智能。他认为如果一台机器能够与人类进行对话而不被人类察觉出其为机器，那么就可以认为这台机器具有智能。这篇论文为人工智能的发展奠定了重要的理论基础。\n弗兰克·罗森布拉特（Frank Rosenblatt） —— 感知机\n1957 年美国心理学家弗兰克·罗森布拉特（Frank Rosenblatt）提出感知机。\n它是一种简单的二分类线性分类模型，它是神经网络和支持向量机的基础。\n感知机的提出在当时引起了很大的轰动，但后来人们发现它只能处理线性可分的问题，具有一定的局限性。不过，感知机的出现为后来更复杂的神经网络的发展奠定了基础。\n20 世纪 60 ~ 70年代初 —— 第一次冬天 计算能力的限制 和 数据的缺乏，机器学习的发展陷入了瓶颈。\n过高的期望未能实现，研究人员和政府资金均转向其他领域。\n20 世纪 80 年代 —— 专家系统（expert system） 专家系统 使得符号主义人工智能热度达到了顶峰。\n20 世纪 90 年代初 —— 第二次冬天 专家系统 维护费用变得很高，难于扩展，并且使用范围有限，人们逐渐对其失去兴趣。\n20 世纪 90 年代 —— 机器学习 专家系统难于给出明确的规则来解决更加复杂、模糊的问题，比如图像分类、语音识别、和语言翻译。\n于是 20 世纪 90 年代出现了 机器学习 来替代符号人工智能。\n机器学习是训练出来的，而不是明确规地用程序编写出来的。\n21 世纪 20 年代 —— 生成式人工智能 OpenAI 在2020年6月发布了 GPT-3，这是一个非常大且复杂的模型，拥有1750亿个参数。\nGPT-3 在生成文本和其他语言任务方面的表现令人瞩目，并且激发了大量关于 AI 的讨论。","2机器学习#\u003cfont face=Georgia\u003e2、机器学习\u003c/font\u003e":"2.1、概述 机器学习 是一门多领域学科，涉及概率论、统计学、逼近论、凸分析、计算复杂性理论等多门学科。 机器学习 主要是设计和分析一些让计算机可以自动“学习”的算法。 机器学习算法 是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。 由于学习算法中涉及了大量的统计学理论，机器学习与推断统计学联系尤为密切，也被称为 统计机器学习。 2.2、分类 （1）监督学习 监督学习：从有标注（标签\\标记）的数据中学习预测模型的机器学习问题；\n监督学习主要任务 —— 分类、回归； 分类任务： 将实例数据划分到预设的某些类别中，是一种离散型预测。如，判断一封电子邮件是不是垃圾邮件；\n回归任务： 用于预测连续值。比如预测房价，股票价格等；\n生成模型 \\ 判别模型： 生成模型 \\ 判别模型： 监督学习的任务就是学习一个模型，应用这个模型，对给定的输入预测相应的输出。\n这个模型的一般形式为决策函数（判别模型 ）： $$Y = f(X)$$ 或者条件概率分布（生成模型）： $$Y = P(Y|X)$$\n判别模型：在已知输入的情况下，预测出输出，常见的判别模型有逻辑回归、支持向量机和深度学习神经网络等。\n生成模型：尝试捕捉数据中的潜在分布，常见的生成模型有高斯混合模型和朴素贝叶斯分类器等。 （2）无监督学习 无监督学习：从无标注（标签\\标记）的、或结构未知的数据中学习预测模型的机器学习问题；\n这种学习方式的目标通常是为了探索数据，找出其中的结构或者模式；\n无监督学习主要任务 —— 聚类、降维、异常检测、关联规则 聚类：目标是发现并识别数据中的自然分组。是构造信息和从数据中导出有意义关系的一种有用的技术； 例如，将客户分为几个群体进行市场细分；为分析过程中出现的每个群定义一组对象，\n它们之间都具有一定程度的相似性，但与其他群中对象的差异性更大；\n降维：特征预处理中数据去噪的一种常用方法，这个过程叫作特征提取。目标是在不丢失太多信息的前提下简化数据； 将多个相关特征合并为一个。降低了某些算法对预测性能的要求，\n并在保留大部分相关信息的同时将数据压缩到较小维数的子空间上。\n案例：汽车的里程与其使用年限存在很大的相关性，所以降维算法会将它们合并成一个代表汽车磨损的特征。\n这个过程叫作特征提取。\n异常检测：异常检测的目标是检测出不符合其他正常数据行为的数据点； 比如，检测信用卡交易中的欺诈行为；\n关联规则：挖掘大量数据，发现属性之间的有趣联系； 比如，在超市销售日志上运行关联规则之后发现买烧烤酱和薯片的人也倾向于购买牛排。那么，可以将这几样商品摆放得更近一些；\n（3）半监督学习 半监督学习：使用标注数据和未标注数据学习预测模型的机器学习问题；通常是大量未标记数据和少量的标记数据；\n半监督学习旨在通过利用大量的未标注数据，帮助提高利用少量标注数据进行监督学习的性能；\n大多数半监督式学习算法是无监督式和监督式算法的结合；例如深度信念网络（DBN），它基于一种互相堆叠的无监督式组件，这个组件叫作受限玻尔兹曼机（RBM）。受限玻尔兹曼机以无监督的方式进行训练，然后使用监督式学习对整个系统进行微调。 （4）强化学习 强化学习：智能系统在与环境的连续互动中学习最有行为策略的机器学习问题；\n它的学习系统（在其语境中称为智能体）能够观察环境，做出选择，执行操作，并获得回报（reward），或者是以负面回报的形式获得惩罚；\n它必须自行学习什么是最好的策略（policy），从而随着时间推移获得最大的回报。策略代表智能体在特定情况下应该选择的操作。；\n例如，许多机器人通过强化学习算法来学习如何行走； （5）主动学习 主动学习： 在训练过程中为模型提供部分数据标签；模型可以选择它认为最需要标签的数据进行标记，从而提高学习效率。\n算法可以主动地选择它想学习的样本，而不是用全部数据进行训练的算法。\n主要思想是，如果允许学习系统选择它自己的训练集，可能会提高学习的效率和效果。它通常用于样本标签成本高、标签难以获得，而未标注样本充足的场景。主动学习可以有效地选择最具信息量、对当前模型最有帮助的样本进行标注，从而减少标注的代价。 （6）自学习 自学习（Self-training）： 自监督学习是通过自动从数据本身中生成监督信号来进行学习。\n这种方法的基本思想是先使用已有的标签数据训练出一个初步的模型，然后用这个模型预测未标签数据，把预测结果作为标签，再和原有标签数据一起重新训练模型。这个过程可以重复进行直到模型收敛。 （7）多视图学习 多视图学习（Multi-view learning）： 在多视图学习中我们假设数据是由多个视图或者说多个不同的特征集生成的，每个视图都可以进行学习和推断，不同视图之间的一致性被用作一种约束以引导学习过程。\n（8）生成模型 生成模型（Generative models）： 生成模型是一类可以描述数据生成过程的模型，它可以同时学习标签数据和未标签数据的分布，并找出两者之间的关联。当有新的数据进来时，他们可以根据学习到的分布生成对应的标签。混合高斯模型和深度生成模型（例如变分自编码器和生成对抗网络）都是这个分类下的具体技术。\n2.3、其他分类 2.3.1、线性\\非线性 线性模型； 线性模型是一种最简单也最重要的统计与机器学习模型。\n在线性模型中，输出是输入特征的线性组合。换句话说，假设特征和输出之间的关系是线性的，即满足一次方程。\n线性模型的一个主要优点是简单易理解、计算效率高，例如线性回归和逻辑回归。\n然而，现实生活中很多问题，特征和输出之间的关系复杂，非线性的。对于这类问题，线性模型的表达能力就显得不够。\n非线性模型； 非线性模型则假设输入与输出之间是非线性关系。\n非线性关系的形式有很多，可能是二次、三次甚至更高次的多项式形式，也可能是指数、对数形式，等等。\n非线性模型的一个主要优点是可以拟合更复杂的关系，例如决策树、神经网络等。\n然而，非线性模型的主要挑战在于可能会过拟合数据（即模型对训练数据学习的过于复杂，不能很好地泛化到新的数据上）、和计算上的复杂性。\n2.3.2、概率\\非概率 概率模型； 概率模型是一种基于概率理论的统计模型，它描述了随机变量之间的概率关系。\n在概率模型中，我们不仅预测某一结果，而且还给出了预测的不确定性（以概率的形式）。\n例如，朴素贝叶斯，高斯混合模型，隐马尔可夫模型，逻辑回归等都是概率模型。\n概率模型的一个主要好处是给出了预测的不确定性，这在许多应用中是非常有用的，如天气预测、医疗诊断等。\n但是概率模型一般假定数据满足某种分布，这种假设在一些情况下可能过于严格，限制了模型的适应性。\n非概率模型； 非概率模型不依赖于先验的可能性模型，而通常依赖于万丽最优化级星拉。\n非概率模型给出明确的预测，不提供预测的不确定性（即预测的概率）。\n例如，支持向量机，决策树，深度学习神经网络等都属于非概率模型。\n非概率模型的优点是无需对数据分布做过多假设，这使得它们更具有适应性，能够处理各种复杂的数据情况。\n然而，非概率模型并不提供预测的不确定性，这在某些情况下可能是一个缺点。\n2.3.3、参数\\非参数 参数模型； 参数模型是一种假设输入和输出之间的关系可以由一组参数完全确定，并且这组参数的数量是固定的。\n参数模型一般都有严格的假设，例如假设数据是正态分布的，或者输入和输出之间的关系是线性的等。\n常见的参数模型包括线性回归、逻辑回归、线性判别分析等。\n参数模型的优点是理论清晰，计算相对简单。\n然而，缺点是对数据的假设可能过于严格，如果实际数据的分布或者结构与模型的假设不符，那么参数模型的表现可能会很差。\n非参数模型； 非参数模型非常灵活，它不假设数据必须符合某一特定的分布，或者输入和输出间的关系必须是某一特定的形式。\n非参数模型的参数数量通常会随着数据量的增加而增加。\n常见的非参数模型包括决策树、随机森林、支持向量机等。\n非参数模型的优点是非常灵活，可以处理各种复杂的数据情况。\n然而，非参数模型的缺点是计算上可能会比较复杂，而且由于模型过于灵活，如果数据量不足，可能会导致过度拟合。\n2.3.4、基于实例\\模型 基于实例的学习； 系统先完全记住学习示例（example），然后通过某种相似度度量方式将其泛化到新的实例。\n基于实例的算法直接记住并使用训练实例进行预测，它们是一种惰性学习算法，因为它们仅在预测时进行计算。\n一个典型例子就是最近邻算法。\n基于实例的学习的优点是可以适应复杂的问题，因为它们不需要进行任何明确的模拟。\n然而，存储和比较所有训练实例通常会造成大量的计算开销。\n基于模型的学习； 在训练过程中会构建一个预测模型，然后使用该模型进行预测。这就是基于模型的学习。\n这种模型可以是线性的（例如线性回归），也可以是非线性的（例如神经网络），或者是更复杂的结构（例如决策树或随机森林）。\n构建模型的敏感过程（例如参数选择，模型选择）通常是自动进行的，并且侧重于提高预测的准确性。\n基于模型的学习的一个主要优点是它们通常可以更有效地处理大量数据。\n然而，选择正确的模型并调整模型参数通常需要专门知识，并可能需要大量时间来得到满意的结果。\n2.3.5、离线\\在线学习 离线学习； 离线学习，又称为批量学习，是最常见的学习方式。\n在离线学习中，我们先从一个固定的训练集中学习模型，然后将该模型应用于新的未知数据。\n由于训练过程是从一个固定的数据集中进行的，因此可以对数据进行多次处理，也就是说可以重复训练。\n此外，训练过程通常需要大量的计算资源，因此通常在专用的机器上进行，并可能需要花费很长时间。\n在线学习； 在线学习是一种连续的学习方式，可以适应新数据的出现。\n在在线学习中，模型以连续的方式接收数据并不断调整，因此能够实时适应新的信息。\n这种学习方式非常适合那些需要不断适应新数据或数据量过大无法一次性处理的场景。\n然而，由于在线学习需要实时进行，因此其计算资源的需求通常比离线学习要更高。\n2.4、简史 （1）概率建模 概率建模 是统计学原理在数据分析中的应用。他是最早机器学习形式之一，其中最有名的算法之一就是 朴素贝叶斯算法。\n朴素贝叶斯，是一类基于应用贝叶斯定理的机器学习分类器，它 假设输入数据的特征都是独立的 。这是一个很强的假设，或者说 “朴素的”假设，其名称正来源于此。这种数据分析方法比计算机出现得还要早，在其第一次被计算机实现（很可能追溯到 20 世纪 50 年代）的几十年前就已经靠人工计算来应用了。贝叶斯定理和统计学基础可以追溯到 18 世纪，你学会了这两点就可以开始使用朴素贝叶斯分类器了。\nlogistic 回归 它有时被认为是现代机器学习的“hello world”。与朴素贝叶斯类似，logistic 的出现也比计算机早很长时间，但由于它既简单又通用，至今仍然很有用。面对一个数据集，数据科学家通常会首先尝试使用这个算法，以便初步熟悉手头的分类任务。\n（2）早期神经网络 早在 20世纪50年代，人们就将神经网络作为玩具项目，并对其核心思想进行研究，但这一方法在数十年后才被人们所使用。在很长一段时间内，一直没有训练大型神经网络的有效方法。这一点在 20 世纪 80年代中期 发生了变化，当时很多人都独立地重新发现了 反向传播算法 —— 一种利用梯度下降优化来训练一系列参数化运算链的方法，并将其应用于神经网络。\n贝尔实验室于 1989 年第一次成功实现了神经网络的实践应用，当时 Yann LeCun将卷积神经网络的早期思想与反向传播算法相结合，并将其应用于手写数字分类问题，由此得到名为 LeNet 的网络，在 20世纪90年代 被美国有证署采用，用于自动读取信封上的邮政编码。\n（3）核方法 虽然神经网络取得了第一次成功，并在 20 世纪 90 年代 开始在研究人员中收到一定重视，但是一种新的机器学习方法在这时声名鹊起，很快就是人们将神经网络抛诸脑后。 这种方法就是 核方法（kernel method）。核方法是一类分类算法，其中最有名的就是 支持向量机（SVM，support vector machine）。 虽然 Vladimir Vapnik 和 Alexey Chervonenkis 早在1963年就发表了较早版本的线性公式，但 SVM 的现代公式有 Vladimir Vapnik 和 Corinna Cortes 于20世纪90年代初期在贝尔实验室提出，并发表于1995年。\n（4）树 决策树（decision tree）是类似于流程图的结构，可以对输入数据点进行分类或根据给定输入来预测输出值。决策树的可视化和解释都很简单。在 21 世纪前十年，从数据中学习得到的决策树开始引起研究人员的广泛关注。到了 2010 年，决策树经常比核方法更受欢迎。\n特别是 随机森林（random forest）算法，它引入了一种健壮且实用的决策树学习方法，即首先构建许多决策树，然后将它们的输出集成在一起。随机森林适用于各种各样的问题——对于任何浅层的机器学习任务来说，它几乎总是第二好的算法。广受欢迎的机器学习竞赛网站 Kaggle 在 2010 年上线后，随机森林迅速成为平台上人们的最爱，直到 2014 年才被梯度提升机所取代。\n与随机森林类似，梯度提升机（gradient boosting machine）也是将弱预测模型（通常是决策树）集成的机器学习技术。它使用了梯度提升方法，通过迭代地训练新模型来专门解决之前模型的弱点，从而改进任何机器学习模型的效果。将梯度提升技术应用于决策树时，得到的模型与随机森林具有相似的性质，但在绝大多数情况下效果都比随机森林要好。它可能是目前处理非感知数据最好的算法之一（如果非要加个“之一”的话）。和深度学习一样，它也是 Kaggle 竞赛中最常用的技术之一。","4统计机器学习#\u003cfont face=Georgia\u003e4、统计机器学习\u003c/font\u003e":"4.1、定义 从给定的数据集合出发，假设数据是独立同分布 产生的：并且假设要学习的 模型属于某个函数的集合 ，称为 假设空间 ；应用某个评价准则，从假设空间中 选取一个最优模型，使它对已知的训练数据及未知的测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。\n统计学习方法包括：模型的假设空间、模型选择的准则 及 模型学习的算法 。 统称为统计学习方法的三要素，简称为 模型、策略 和 算法。 4.2、三要素 （1）模型： 在监督学习过程中，模型就是所要学习的 条件概率分布 或 决策函数。模型的 假设空间 包含所有可能的 条件概率分布 或 决策函数。例如，假设决策函数是输入变量的线性函数，那么模型的假设空间就是所有这些线性函数构成的函数集合。假设空间一般有无穷多个。\n决策函数： 假设假设空间用 $\\mathcal{F}$ 表示。假设空间可以定义为 决策函数的集合：\n$$\\mathcal{F}={\\begin{array}{c}f\\mid Y=f(X)\\end{array}}$$\n其中：\n$X$ 和 $Y$ 是定义在输入空间 $\\mathcal{X}$ 和输出空间 $\\mathcal{Y}$ 上的变量。 这时 $\\mathcal{F}$ 通常是由一个参数向量决定的函数族： ${\\mathcal F}={f|Y=f_{\\theta}(X),\\theta\\in\\mathbf{R}^{n}}$。 参数向量 $\\theta$ 取值于 $n$ 维欧式空间 $\\mathbf{R}^{n}$ ，称为参数空间。 条件概率分布：假设空间 $\\mathcal{F}$ 也可以定义为 条件概率的集合：\n$$\\mathcal{F}={P\\mid P(Y|X)}$$\n其中：\n$X$ 和 $Y$ 是定义在输入空间 $\\mathcal{X}$ 和输出空间 $\\mathcal{Y}$ 上的随机变量。 这时 $\\mathcal{F}$ 通常是由一个参数向量决定的条件概率分布族： ${\\mathcal F}={P\\mid P_{\\theta}(Y|X),\\theta\\in\\mathbf{R}^{n}}$。 参数向量 $\\theta$ 取值于 $n$ 维欧式空间 $\\mathbf{R}^{n}$ ，称为参数空间。 （2）策略： 策略就是从模型的假设空间中，选择损失函数值最小的模型。 使用损失函数(loss function) 或 代价函数(cost function) 来衡量模型预测的错误程度。 损失函数值越小，模型越好。\n1. 损失函数：(loss function)： 损失函数是 $f(X)$ 和 $Y$ 的非负实值函数，记作 $L(Y,f(X))$。 常见的损失函数如下： (1). 0-1 损失函数 （0-1 loss function）： $$\\small L(Y,f(X))=\\begin{cases}1,\u0026Y\\neq f(X)\\\\ 0,\u0026Y=f(X)\\end{cases} \\tag{1}$$\n(2). 平方损失函数 （quadratic loss function）： $$\\small L(Y,f(X))=(Y-f(X))^{2} \\tag{2}$$\n(3). 绝对损失函数 （absolute loss function）： $$\\small L(Y,f(X))=|Y-f(X)| \\tag{3}$$\n(4). 对数损失函数 （logarithmic loss function）： $$L(Y,P(Y|X))=-\\log P(Y|X) \\tag{4}$$\n其中：$P(Y|X)$ 是条件概率分布，$-\\log P(Y|X)$ 是对数似然函数。 2. 风险函数：(risk function)： 由于模型的输入、输出 $（X, Y）$ 是随机变量，遵循联合分布 $P(X, Y)$，所以损失函数的期望 $\\small R_{\\exp}$ 如下；这是理论上模型 $f(X)$ 关于联合分布 $P(X, Y)$ 的平均损失，称为 风险函数 (risk function) 或 期望损失 (expected loss)。 $$\\small R_{\\exp}(f)=E_{P}[L(Y,f(X))]=\\int_{x\\times y}L(y,f(x))P(x,y)\\mathrm{d}x\\mathrm{d}y$$ 由于联合分布 $P(X,Y)$ 未知，所以无法直接计算期望风险 $\\small R_{\\exp}(f)$。 学习的目标（策略）就是选择期望风险最小的模型。 3. 经验风险：(empirical risk)： 给定一个训练数据集 $T={(x_{1},y_{1}),(x_{2},y_{2}),\\cdots,(x_{N},y_{N})}$，模型 $f(X)$ 在训练数据集上的平均损失称为 经验风险\\损失，记作$\\small R_{emp}(f)$： $$R_{\\mathrm{emp}}(f)=\\frac{1}{N}\\sum_{i=l}^{N}L(y_{i},f(x_{i}))$$\n根据大数定律，当样本数 $N$ 足够大时，经验风险 $\\small R_{emp}(f)$ 收敛于期望风险 $\\small R_{\\exp}(f)$。所以能用经验风险估计期望风险。\n由于现实中训练样本数目有限，甚至很小，所以用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。这就关系到监督学习的两个基本策略:经验风险最小化 和 结构风险最小化。\n❓ 大数定理 是概率论中一系列定理的统称，最常见的表述为： 随着样本数量的增加，样本均值会趋近于总体均值。更严格地说，如果 $X_1, X_2, … , X_n$ 是一系列独立同分布的随机变量，且它们的期望为 $\\mu$，方差为 $\\sigma^2$，那么当 $n$ 趋向于无穷大时，样本均值 ${\\overline{X}}={\\frac{1}{n}}\\sum_{i=1}^{n}X_{i}$ 依概率收敛于 $\\mu$，即对于任意正数 $\\epsilon$ ，有： $$\\lim_{n\\to\\infty}P\\left(\\left|\\overline{X}-\\mu\\right|\u003c\\epsilon\\right)=1$$ 简单来说，大数定理表明在大量重复试验中，事件发生的频率会趋近于其概率。它是概率论和统计学中非常重要的基础性定理，为用样本估计总体提供了理论依据。 4. 经验风险最小化 (empirical risk minimization，ERM) 在假设空间、损失函数以及训练数据集确定的情况下，经验风险函数就可以确定。 经验风险最小化 (empirical risk minimization，ERM) 的策略认为，经验风险最小的模型是最优的模型。根据这一策略，按照经验风险最小化求最优模型就是 求解最优化问题： $$\\min_{f\\in\\mathcal{F}}\\frac{1}{N}\\sum_{i=1}^NL(y_i,f(x_i))$$ 其中，$\\mathcal{F}$ 是假设空间。\n当样本容量足够大时，经验风险最小化能保证有很好的学习效果，在现实中被广泛采用。\n比如，极大似然估计 (maximumlikelihood estimation) 就是经验风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数时，经验风险最小化就等价于极大似然估计。\n当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生“过拟合”(over-ftting) 现象。 5. 结构风险最小化 (structural risk minimization，SRM) 结构风险最小化 (structural risk minimization，SRM) 是为了防止过拟合而提出来的策略。结构风险最小化等价于正则化 (regularization)。结构风险在经验风险上加上表示模型复杂度的正则化项 (regularizer) 或罚项 (penaltyterm)。 在假设空间、损失函数以及训练数据集确定的情况下， 结构风险的定义 是: $$R_{\\mathrm{srm}}(f)=\\frac1N\\sum_{i=1}^NL(y_i,f(x_i))+\\lambda J(f)$$ 其中 $J(f)$ 为模型的复杂度，是定义在假设空间 $\\mathcal{F}$ 下上的泛函。 模型 $f$ 越复杂，复杂度 $J(f)$ 就越大;反之，模型 $f$ 越简单，复杂度 $J(f)$ 就越小。 也就是说，复杂度表示了对复杂模型的惩罚。$\\lambda≥0$ 是系数,用以权衡经验风险和模型复杂度。 结构风险小需要经验风险与模型复杂度同时小。结构风险小的模型往往对训练数据以及未知的测试数据都有较好的预测。 比如，贝叶斯估计中的最大后验概率估计 (maximum posterior probability esti-mation，MAP) 就是结构风险最小化的一个例子。当模型是条件概率分布、损失函数是对数损失函数、模型复杂度由模型的先验概率表示时，结构风险最小化就等价于最大后验概率估计。\n结构风险最小化的策略认为结构风险最小的模型是最优的模型。所以求最优模型，就是求解最优化问题: $$\\min_{f\\in\\mathcal{F}}\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,f(x_i))+\\lambda J(f)$$ 这样，监督学习问题就变成了 经验风险 或 结构风险函数 的最优化问题。这时 经验或结构风险函数 是最优化的目标函数。 （3）算法： 算法是指学习模型的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么样的计算方法求解最优模型。\n这时，统计学习问题归结为最优化问题，统计学习的算法成为求解最优化问题的算法。如果最优化问题有显式的解析解，这个最优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。如何保证找到全局最优解，并使求解的过程非常高效，就成为一个重要问题。统计学习可以利用已有的最优化算法，有时也需要开发独自的最优化算法。\n统计学习方法之间的不同，主要来自其模型、策略、算法的不同。确定了模型、策略、算法，统计学习的方法也就确定了。这就是将其称为统计学习方法三要素的原因。\n4.3. 步骤： 得到一个有限的训练数据集合； 确定包含所有可能的模型的假设空间，即学习 模型的集合 （条件概率分布 或 决策函数）； 确定模型选择的准则，即学习的 策略（损失函数）； 实现求解最优模型的算法，即学习的算法； 通过学习方法选择最优模型； 利用学习的最优模型对新数据进行预测或分析； 4.4. 统计机器学习 VS 统计学 统计学 的最新进展均致力于为回归和分类提供更强大的自动预测建模技术。这些方法都属于统计机器学习。 不同于经典的统计方法，统计机器学习 是数据驱动的，并不试图在数据上强加线性结构或其他的整体结构。\n不需要过多关注数据分布、假设检验，数据泛化能力更有用。 最近邻算法就是一种非常简单的方法，它根据与一个记录相似的记录的分类情况，对该记录进行分类。 最为成功的技术，是结合决策树的集成学习。集成学习的基本理念是，运用多个模型而非单一模型去生成预测。 决策树对于学习预测变量和结果变量之间关系的规则，是一种灵活且自动化的技术。 事实证明，集成学习与决策树相结合，可以得到性能优良的、可用的预测建模技术。 4.5. 统计机器学习 VS 机器学习 二者之间并不存在一条明确的分界线。 机器学习更关注 如何开发可扩展到大规模数据上的高效算法，以便于优化预测模型。 统计学更关注的是 概率理论和模型的底层结构。 Bagging算法 和 随机森林 方法完全是从统计学领域发展出来的。 Boosting方法 从这两个学科中发展起来的，只是在机器学习一方得到了更多的关注。 如果不看历史的话，Boosting 的发展确保了该技术同时适用于统计学和机器学习这两个领域。 ","5模型评估选择#\u003cfont face=Georgia\u003e5、模型评估选择\u003c/font\u003e":"5.1、模型泛化 模型泛化 指的是模型对新的、未曾见过的数据的适应能力和预测能力。 一个具有良好泛化能力的模型，不仅在训练数据上表现出色，在面对新的、来自相同分布的测试数据或实际应用中的数据时，也能给出准确和可靠的预测结果。\n如果模型 过度拟合 训练数据，会记住训练数据中的噪声和特定细节，导致泛化能力差，在新数据上表现不佳；而如果模型 欠拟合 训练数据，则无法充分学习到数据中的模式和规律，同样泛化能力不佳。\n评估模型泛化能力的常见方式是在独立的测试集上对模型进行测试，并观察其性能指标。\n5.2、过拟合 过拟合$（Overfitting）$ 指模型在训练数据上表现得非常好，但在新的、未见过的数据上表现很差。 通常是因为 模型过于复杂，学习到了训练数据中的噪声和一些特定的、不具有普遍性的特征，而不是真正的数据模式和规律。 过拟合的 表现包括 ：在训练集上准确率很高，但在测试集上准确率显著下降；模型对训练数据的微小变化非常敏感。 导致过拟合的 原因可能有 ：数据量过少、模型过于复杂（例如层数过多、参数过多）、训练时间过长等。\n处理过拟合的方法主要有以下几种： 增加数据量：收集更多的相关数据，以丰富模型的学习内容，使其能够更好地捕捉数据的普遍规律，而不是局限于训练数据中的噪声和特殊情况。 数据增强：通过对现有数据进行随机变换，如翻转、旋转、缩放、添加噪声等，增加数据的多样性。 简化模型：减少模型的复杂度，例如减少网络层数、神经元数量，或者使用更简单的模型结构。 正则化： L1 和 L2 正则化：在损失函数中添加对模型参数的惩罚项，限制模型参数的大小，防止过度拟合。 Dropout：在训练过程中随机地将神经元的输出设置为 0，强迫模型学习更鲁棒的特征表示。 早停法\\Early Stopping：在训练过程中，持续监测模型在验证集上的性能，当性能不再提升时提前停止训练。 集成学习：例如使用随机森林等集成学习方法，通过组合多个模型来降低过拟合的风险。 特征选择：去除一些不重要或冗余的特征，降低模型学习的难度。 交叉验证：使用多种交叉验证方法，如 K 折交叉验证，更准确地评估模型性能，并选择合适的超参数。 5.3、欠拟合 欠拟合$（Underfitting）$ 指模型在训练数据和测试数据上的表现都不好。这意味着模型没有充分学习到数据中的模式和规律，过于简单或者训练不充分。 欠拟合的 表现是 模型在训练集和测试集上的误差都较大，且两者的差距较小。 导致欠拟合的 原因可能是 ：模型的能力不足、特征选择不当、训练次数太少等。\n处理欠拟合的方法通常包括以下几种： 增加模型复杂度： 对于神经网络，可以增加层数或神经元数量。 对于传统机器学习算法，例如决策树，可以增加树的深度。 增加特征数量：尝试引入更多相关的特征，以提供更多信息给模型进行学习。 选择更强大的模型：例如从线性模型切换到非线性模型，如使用支持向量机、决策树或神经网络等。 调整超参数：通过试验不同的超参数组合，找到最适合当前数据和任务的设置。 延长训练时间：确保模型有足够的时间来学习数据中的模式。 数据预处理：对数据进行更有效的清洗、归一化、标准化等处理，以改善数据质量和模型的学习效果。 组合多个模型：通过集成多个弱学习器来构建一个更强的学习器。 尝试新的算法：如果当前使用的算法效果不佳，可以尝试其他适合该任务和数据特点的算法。 5.4、评估指标 （1）分类评估指标 1、混淆矩阵： 真实值 总数 $\\pmb{\\it{p}} \\,(positive)$ $\\pmb{\\it{n}}\\,(negative)$ 预测值 $\\pmb{\\it{p'}}\\,(positive)$ TP FP P' $\\pmb{\\it{n'}}\\,(negative)$ FN TN N' 总数 P N 阳性 $(P, positive)$ 阴性 $(N, Negative)$ 真阳性 $(TP, true positive)$ 正确的肯定。又称：命中 $(hit)$ 真阴性 $(TN, true negative)$ 正确的否定。又称：正确拒绝 $(correct rejection)$ 伪阳性 $(FP, false positive)$ 错误的肯定，又称：假警报 $(false alarm)$，第一型错误 伪阴性 $(FN, false negative)$ 错误的否定，又称：未命中 $(miss)$，第二型错误 优点： 提供了关于分类模型在各个类别上的详细分类情况，包括正确分类和错误分类的数量。 直观易懂，能够清晰地展示模型的分类效果。 缺点： 对于多分类问题，混淆矩阵可能会变得非常复杂和庞大，不太容易直观地理解。 单独的混淆矩阵本身并不能直接给出一个单一的综合评估指标。 适用情况： 当需要详细了解模型在每个类别上的分类表现时，混淆矩阵非常有用。 在比较不同分类模型或调整模型参数时，可以通过分析混淆矩阵来洞察模型的行为和改进方向。对于不平衡数据集（某些类别样本数量远多于或少于其他类别），混淆矩阵可以帮助发现模型对少数类别的分类效果。 2、准确率$（Accuracy）$： 准确率 $（Accuracy）$：正确预测的样本数占总样本数的比例。公式为： $$Accuracy = \\frac{(TP + TN)}{(TP + TN + FP + FN)}$$\n优点：\n直观易懂：是一个非常直观和易于理解的指标，能够简单地反映模型整体分类正确的比例。 综合性：综合考虑了所有类别的分类情况。 缺点： 类别不平衡问题：在数据存在类别不平衡（某些类别样本数量远多于或远少于其他类别）时，准确率可能会产生误导。例如，在一个二分类问题中，99%的样本属于类别 A，1%属于类别 B，如果模型总是预测为类别 A，准确率也能达到 99%，但实际上对类别 B 的分类效果很差。 不能区分不同类别的错误情况：无法提供关于每个类别分类错误的具体信息，可能掩盖了某些重要类别上的糟糕表现。 缺乏细节：不能反映模型在不同类别上的性能差异，对于需要深入了解模型在各个类别上表现的情况，准确率提供的信息有限。 3、精确率$（Precision）$： 精确率$（Precision）$： 在被预测为正例的样本中，真正的正例所占的比例。公式为： $$Precision = \\frac{TP}{TP + FP}$$\n优点：\n聚焦正例的准确性：精确率重点关注被模型预测为正例的样本中，真正为正例的比例。这在一些对正例预测的准确性要求较高的场景中非常有用，例如在疾病诊断中，确保被诊断为患病的人确实患病是至关重要的。 适用于类别不平衡数据：在正例样本相对较少的不平衡数据集中，精确率能够更有针对性地评估模型对正例的预测能力。 缺点：\n忽略了负例的情况：精确率只考虑了被预测为正例的样本，而完全忽略了被预测为负例的样本，可能会导致对模型整体性能的评估不全面。 孤立的评估指标：单独依靠精确率不能完全反映模型的综合性能，往往需要结合其他指标如召回率、F1 值等来进行更全面的评估。 受阈值影响：精确率的值会受到分类阈值的选择影响，不同的阈值可能会导致精确率的变化，这使得在比较不同模型或方法时需要谨慎处理阈值的设置。 4、召回率$（Recall）$、命中率 $(hit rate)$、敏感度 $(sensitivity)$： 召回率$（Recall）$： 实际的正例样本中，被正确预测为正例的比例。公式为： $$Recall = \\frac{TP}{TP + FN}$$\n优点：\n强调对正例的全面覆盖：召回率关注的是在所有实际为正例的样本中，被正确预测为正例的比例。这对于一些重视不能遗漏正例的任务非常重要，比如疾病检测，确保尽可能多地发现真正的患者。 适用于不平衡数据：在数据集中正例相对较少的情况下，召回率能够反映出模型在捕捉少数类（正例）方面的能力。 缺点：\n可能忽视误报：高召回率可能是以增加误报（将负例错误地预测为正例）为代价的，而召回率本身并不能直接反映这一情况。 单独使用不够全面：仅依靠召回率来评估模型的性能可能不够全面，通常需要结合准确率、F1 值等其他指标进行综合评估。 对负例的判断不敏感：召回率主要关注正例的预测情况，对于负例的准确预测情况反映较少。 5、F1 分数$（F1-score）$： F1 分数$（F1-score）$： 是精确率和召回率的调和平均数，综合考虑了两者的平衡。一般式为： $$F_\\beta=(1+\\beta^2)\\cdot\\frac{\\text{precision}\\cdot\\text{recall}}{(\\beta^2\\cdot\\text{precision})+\\text{recall}}$$ $\\beta$ 是使用者自行定义的参数，由一般式可见 $F-score$ 能同时考虑 $precision$ 和 $recall$ 这两种数值。分子为 $precision$ 和 $recall$ 相乘，根据这个式子，只要 $precision$ 或 $recall$ 趋近于 0，$F-score$ 就会趋近于0，代表着这个算法的精确度非常低。一个好的算法，最好能够平衡 $recall$ 和 $precision$ ，且尽量让两种指标都很高。所以有一套判断方式可以同时考虑 $recall$ 和 $precision$ 。当 $\\beta\\to0$ 时，$F-score$ 退化为 $precision$ ；当 $\\beta\\rightarrow\\infty$ 时，$F-score$ 退化为 $recall$。\n$Precision$ 和 $Recall$ 权重一样时：一般上来说，提到 $F-score$ 且没有特别的定义时，是指 $\\beta=1$ 时的 $F-score$，亦有写作 $F1-score$。代表使用者同样的注重 $precision$ 和 $recall$ 的这两个指标。其分数可以说是 $precision$ 和 $recall$ 的调和平均，式子如下：\n$$F_1=\\frac{2}{\\text{recall}^{-1}+\\text{precision}^{-1}}=2\\frac{\\text{precision}\\cdot\\text{recall}}{\\text{precision}+\\text{recall}}=\\frac{2TP}{2TP+FP+FN}$$\n$F-score$ 最理想的数值是趋近于1，做法是让 $precision$ 和 $recall$ 都有很高的值。若两者皆为1，使得 $2*\\frac{1}{2}=1$，则 $F-score = 1$ （100%），代表该算法有着最佳的精确度。\n优点： 综合考虑了准确率和召回率：F1 分数是准确率和召回率的调和平均数，能够平衡模型在这两个方面的表现，对于需要同时关注查准和查全的任务非常有用。 适用于不平衡数据：在正例和负例数量差异较大的情况下，F1 分数能够更全面地评估模型性能，避免只侧重准确率或召回率中的某一个。 单一数值便于比较：F1 分数是一个单一的数值，便于在不同模型或不同参数设置之间进行比较和选择。 缺点： 对极端值敏感：当准确率或召回率中有一个非常接近 0 时，F1 分数会受到较大影响。 可能掩盖某些问题：虽然综合了准确率和召回率，但不能直接反映出模型在这两个指标上的具体差异和潜在的问题。 不完全反映实际需求：在某些特定场景中，可能更关注准确率或者召回率中的某一个，而 F1 分数不能完全突出这种特定的需求。 （2）多标签分类评估 5.5、评估方法 （1）留出法 留出法 $（Hold-out Method）$ 是一种常见的机器学习模型评估方法。\n基本思想：\n将数据集划分为两个互斥的集合，分别是 训练集 和 测试集。通常，大部分数据被分配到训练集，用于模型的学习和训练；小部分数据被分配到测试集，用于评估训练好的模型的性能。\n划分要具有随机性，以避免数据的偏差。 训练集和测试集的样本分布应尽量与原始数据集的分布保持一致。注意通过关键分类指标进行分层拆分。 通常，将大约 70%-80% 的数据作为训练集，20%-30% 的数据作为测试集，但具体比例可以根据数据规模和实际需求进行调整。 如果需要查看模型在新数据上的表现情况，或是模型需要调整参数，也可以在 验证集 上测试。\n将数据集划分为三个互斥的集合，分别是 训练集，测试集 和 验证集， 比例可以为 60%，20%，20% 优点：\n简单、直接； 计算开销小； 缺点：\n评估结果可能不稳定； 受划分数据随机性影响较大 （2）交叉验证 交叉验证（Cross Validation）是一种用于评估机器学习模型性能和选择合适模型超参数的技术。如，常见的 K 折交叉验证。\nK 折交叉验证原理：\n将数据集随机平均分成 K 份。每次选择其中 1 份作为测试集，其余 K - 1 份作为训练集，重复 K 次，这样每个子集都有机会作为测试集，最终得到 K 个模型的评估结果。综合这 K 个结果来估计模型的泛化能力。\n例如，对于 5 折交叉验证，数据被分为 5 份。第一次，第 1 份作为测试集，其余 4 份作为训练集；第二次，第 2 份作为测试集，其余 4 份作为训练集，以此类推，共进行 5 次训练和测试。\n优点：\n更充分地利用了数据，减少了由于数据划分随机性导致的偏差。 可以用于评估模型在不同数据子集上的稳定性和泛化能力。 缺点：\n计算成本相对较高，特别是当 K 值较大或数据集较大时。 对于时间开销较大的模型，可能会耗费较多的时间。 交叉验证在模型选择、超参数调优、评估模型稳定性等方面都有广泛的应用。 （3）留一法 留一法（Leave-One-Out，LOO）是一种特殊的交叉验证方法。在留一法中，每次从样本集中取出一个样本作为测试集，其余的样本作为训练集，重复这个过程直到所有的样本都被用作一次测试样本。\n假设样本集有 $n$ 个样本，那么就会进行 $n$ 次训练和测试。在每次迭代中，将其中一个样本作为测试样本，其余 $n-1$ 个样本作为训练样本进行模型训练，然后用训练好的模型对那个被取出的样本进行预测，并记录预测结果。最后，综合这 $n$ 次预测结果来评估模型的性能。\n优点：\n充分利用数据 由于每次只留下一个样本进行测试，几乎使用了所有的样本进行训练，因此对于小样本数据集来说，可以最大限度地利用数据信息，使得评估结果更加准确。\n无偏估计 留一法的评估结果通常是无偏的，因为每个样本都被单独用作测试集，避免了随机划分训练集和测试集可能带来的偏差。\n缺点：\n计算成本高 由于需要进行 次训练和测试，当样本数量较大时，计算量会非常大，导致训练和评估过程非常耗时。\n不一定反映真实性能 在某些情况下，留一法可能会过于乐观地估计模型的性能，因为每次训练集和测试集之间的差异非常小，模型可能会过度拟合到特定的测试样本。而且，由于每次只改变一个样本进行测试，可能无法很好地反映模型在不同数据分布下的泛化能力。","参考网址#\u003cfont face=Georgia\u003e参考网址：\u003c/font\u003e":" Training | Microsoft Learn — 基本 AI 概念 《Python深度学习》- 第2版 - 弗朗索瓦·肖莱 《机器学习实战：基于 Scikit-Learn、Keras 和 TensorFlow》 第2版 《统计学习方法》 第2版 - 李航 "},"title":"1️⃣ 概述"},"/ai/end_to_end/tool/":{"data":{"":"\nPandas一个快速、强大、灵活且易于使用的开源数据分析和操作工具。 数据分析 SklearnPython 中的机器学习；简单有效的预测数据分析工具。 机器学习 Matplotlib用 Python 创建静态、动画和交互式可视化。 "},"title":"2️⃣ 工具"},"/blog/":{"data":{"":" 数据科学实践 关于 AI，大数据的学习笔记，项目实践 👻 关于 ✍ 项目 ⚓ 指南 人工智能 机器学习，深度学习，强化学习，计算机视觉，自然语言处理，MLOps，生成式人工智能，通用人工智能…..\n大数据 传输，存储，计算，展示；Flink，Kafka, DolphinScheduler，Zeppelin，Doris，Hudi，Datart……\n物联网 …..\n数据分析方法….. 端到端的机器学习简单易用，功能强大丰富。 核心框架简单易用，功能强大丰富。 商业分析方法….. 深度学习简单易用，功能强大丰富。 数据仓库B 站，数仓项目学习（尚硅谷，等）； 可视化Python，BI 生成式人工智能大模型 数据治理简单易用，功能强大丰富。 Python\\SQL 数据分析Datart + Python\\SQL 分布式训练简单易用，功能强大丰富。 模型部署 MLOps简单易用，功能强大丰富。 "},"title":"项目列表"},"/guide/":{"data":{"":"","1数学知识#1、数学知识":" 微积分是研究变化\\微分与累积\\积分的数学分支，用于解决运动、曲线、面积等动态问题。 AI 基础 线性代数是研究向量、矩阵和线性变换的数学分支，核心用来解方程、处理空间变换和数据分析。 AI 基础 概率论是研究随机现象规律性的数学分支，用概率量化不确定性。 AI 基础 数理统计是用数学工具（尤其是概率论）从数据中总结规律、预测未知的学科，核心是抽样、估计和假设检验。 AI 基础 ","2编程语言#2、编程语言":" Linux\\ShellLinux 是一种自由和开放源码的类 UNIX 操作系统。 大数据\\云计算基础 Python一种广泛使用的解释型、高级和通用的编程语言。 AI 基础 Java由 Sun 公司于 1995 年 5 月推出的高级程序设计语言。 大数据基础 Scala一种可随您扩展的编程语言：从小型脚本到大型多平台应用程序。 大数据基础 Go一种快速、静态类型的编译语言，感觉就像是一种动态类型的解释语言。 云原生基础 ","3机器学习python#3、机器学习：Python":"3.1、科学计算\\建模 NumpyPython 科学计算的基本包；强大的 N 维数组；数值计算工具。 数据分析 Pandas一个快速、强大、灵活且易于使用的开源数据分析和操作工具。 数据分析 SklearnPython 中的机器学习；简单有效的预测数据分析工具。 机器学习 Sklearn 源码Sklearn 源码\n机器学习 StatsmodelsPython 统计建模库；用于进行统计测试和统计数据探索。 统计模型 3.2、可视化 Matplotlib用 Python 创建静态、动画和交互式可视化。 Seaborn一个基于 matplotlib 的 Python 数据可视化库。 PlotlyPlotly 的 Python 图形库可制作交互式、出版质量的图形。 3.3、模型解释 SHAP一种博弈论方法，用于解释任何机器学习模型的输出。 3.4、模型部署 Streamlit一个开源 Python 框架；快速构建和部署强大的数据应用程序。 FlaskFlask 是一个轻量级的 WSGI Web 应用程序框架。 3.5、空间数据 GeoPandas旨在让使用 Python 处理地理空间数据变得更加容易。 Folium轻松地在交互式传单地图上可视化使用 Python 处理的数据。 3.6、自动机器学习 ","4深度学习#4、深度学习":" Keras一个用 Python 编写的开源神经网络库。 TensorFlowGoogle Brain 团队开发的一个机器学习和深度学习的开源库。 TensorFlow 源码Tensorflow 源码 FastAIfastai 是一个深度学习库，易于使用且快速高效。 PyTorch一个基于 Python 的开源机器学习库。 PyTorch 源码PyTorch 源码\n…… DeZero使用 Python 从头开始实现深度神经网络。 ","5数据库#5、数据库":" MySQL一个流行的关系型数据库管理系统。 PostgreSQL一个功能强大的开源对象关系数据库系统。 ","5软件工程#5、软件工程":" GitGit 是一个免费的开源 分布式版本控制系统。 Github一个在线软件源代码托管服务平台，使用Git作为版本控制软件。 Docker一个开源的应用容器引擎，基于 Go 语言。 云原生 Kubernetes开源容器编排引擎，用于自动部署、扩展和管理容器化应用程序 云原生 ","6商业分析bi#6、商业分析：BI":" Datart新一代数据可视化开放平台。 观远 BI让业务用起来的现代化BI。 Quick BI一款全场景数据消费式的BI平台。 PowerBI由微软开发的商业分析工具集。 Tableau一款强大的数据可视化工具。 ","7大数据#7、大数据":"7.1、ETL 抽取\\转换\\加载 SeaTunnel下一代高性能 / 分布式、海量数据集成工具。\nWaterdrop（现更名为 SeaTunnel） 支持批流一体 Flink CDC一个基于流的数据集成工具，旨在为用户提供一套功能更加全面的编程接口（API）。 抽取 Flume一种分布式、可靠且可用的服务，用于高效收集、聚合和移动大量日志数据。 日志抽取\\加载 7.2、存储\\数据湖 IcebergApache Iceberg 是一种适用于大型分析表的高性能格式。 多引擎兼容 7.3、计算\\处理 Hadoop一个框架，允许使用简单的编程模型跨计算机集群分布式处理大型数据集。 存储\\批处理 HiveApache Hive 是一个分布式、容错数据仓库系统，可实现大规模分析。 批处理 Spark一个多语言引擎，用于在单节点机器或集群上执行数据工程、数据科学和机器学习。 批处理 Flink一个框架和分布式处理引擎，用于对无界和有界数据流进行有状态计算。 流处理 Kafka一个开源分布式事件流平台，实现高性能数据管道、流分析、数据集成和关键任务应用程序。 消息队列 + 流处理 Doris一个用于实时分析的现代数据仓库。它可以对大规模实时数据进行闪电般快速的分析。 OLAP StarRocks一款高性能分析型数据仓库，可进行多维、实时、高并发的数据分析。 OLAP 7.4、调度 Airflow一个由社区创建的平台，用于以编程方式编写、安排和监控工作流程。 DolphinScheduler一个多元化、可扩展的开源工作流协调平台，具有强大的DAG可视化界面。 7.5、开发\\IDE StreamPark一个用户友好的流媒体应用程序开发框架和一站式云原生实时计算平台。 Zeppelin基于 Web 的笔记本，支持使用 SQL、Scala、Python、R 等进行 数据驱动、交互式数据分析和协作文档。 7.6、治理：元数据\\血缘\\目录 DataHub 一个现代数据目录，旨在简化元数据管理、数据发现和数据治理。 数据目录\\血缘\\元数据 OpenMetadata一个统一的元数据平台，用于数据发现、数据可观测性和数据治理，由中央元数据存储库、深入的列级沿袭和无缝团队协作提供支持。 数据目录\\血缘\\质量\\元数据 7.7、治理：数据质量 Great Expectations批量和流式大数据质量解决方案。 Python/现代数据栈 Soda Core一个免费的开源 Python 库和 CLI 工具，使数据工程师能够测试数据质量。 低代码 7.8、治理：数据安全 Open Policy Agent一个开源的通用策略引擎，可在整个堆栈中实现统一的上下文感知策略实施。 云原生 Ranger一个框架，用于在整个 Hadoop 平台上启用、监控和管理全面的数据安全性。 HDFS/Hive/Kafka "},"title":"指南"},"/guide/math_calculus/":{"data":{"":"","good#good":"test "},"title":"微积分"},"/guide/math_linear_algebra/":{"data":{"":"","#":"1、定义 1.1、一般形式：\n一个 $m\\times n$ 的矩阵 $\\boldsymbol{A}$ 可以表示为： $\\boldsymbol{A}=\\begin{bmatrix}a_{11}\u0026a_{12}\u0026\\cdots\u0026a_{1n} \\\\ a_{21}\u0026a_{22}\u0026\\cdots\u0026a_{2n} \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ a_{m1}\u0026a_{m2}\u0026\\cdots\u0026a_{mn}\\end{bmatrix}$ ，其中 $a_{ij}$ 或 $\\boldsymbol{A}_{ij}$ 表示矩阵 $\\boldsymbol{A}$ 的第 $i$ 行第 $j$ 列的元素。 1.2、特殊矩阵：\n方阵：当矩阵的行数 $m$ 等于列数 $n$ 时，称为 $n$ 阶方阵 零矩阵：所有元素都为 0 的矩阵，记为 $\\boldsymbol{O}$ 单位矩阵：主对角线元素都为 1，其余元素都为 0 的方阵，记为 $\\boldsymbol{I}$（或 $\\boldsymbol{E}$ ）。对于 $n$ 阶单位矩阵，通常表示为 $\\boldsymbol{I}_n$ 2、基本运算 2.1、数乘：\n标量 $c$ 与矩阵 $\\boldsymbol{A}$ 的数乘，$c\\boldsymbol{A}$ 中的元素是 $\\boldsymbol{A}$ 的对应元素与 $c$ 的乘积。 $$2\\cdot\\begin{bmatrix}2\u00268\u0026-3 \\\\ 6\u0026-2\u00265\\end{bmatrix}=\\begin{bmatrix}2\\cdot2\u00262\\cdot8\u00262\\cdot(-3) \\\\ 2\\cdot6\u00262\\cdot(-2)\u00262\\cdot5\\end{bmatrix}=\\begin{bmatrix}4\u002616\u0026-6 \\\\ 12\u0026-4\u002610\\end{bmatrix}$$ 2.2、转置：\n一个 $m \\times n$ 的矩阵 $\\boldsymbol{A}$ 的转置，是一个 $n\\times m$ 的矩阵，记作 $\\boldsymbol{A}^T$，转置矩阵 $\\boldsymbol{A}^T$ 第 $i$ 行第 $j$ 列的元素是原矩阵 $\\boldsymbol{A}$ 第 $j$ 行第 $i$ 列的元素（行列互换）。 $$\\begin{bmatrix}1\u00264\u00263 \\\\ 0\u0026-5\u00267\\end{bmatrix}^T=\\begin{bmatrix}1\u00260 \\\\ 4\u0026-5 \\\\ 3\u00267\\end{bmatrix}$$ $$c(\\boldsymbol{A}^T) = c(\\boldsymbol{A})^T$$ 2.3、加（减）法：\n$m \\times n$ 矩阵 $\\boldsymbol{A}$ 和 $\\boldsymbol{B}$ 的加（减）：$\\boldsymbol{A} \\pm \\boldsymbol{B} = \\boldsymbol{C}$ 为一个 $m\\times n$ 矩阵，其中 $\\boldsymbol{C}$ 中的元素是 $\\boldsymbol{A}$ 和 $\\boldsymbol{B}$ 相应元素的加（减）：$c_{ij} = a_{ij} + b_{ij}$，$1 \\leq i \\leq m$，$1 \\leq j \\leq n$。 $$\\begin{bmatrix}1\u00263\u00261 \\\\ 1\u00260\u00260\\end{bmatrix}+\\begin{bmatrix}1\u00260\u00265 \\\\ 7\u00265\u00260\\end{bmatrix}=\\begin{bmatrix}1+1\u00263+0\u00261+5 \\\\ 1+7\u00260+5\u00260+1\\end{bmatrix}=\\begin{bmatrix}2\u00263\u00266 \\\\ 8\u00265\u00260\\end{bmatrix}$$ $$\\boldsymbol{A} + \\boldsymbol{B} = \\boldsymbol{B} + \\boldsymbol{A}$$ $$(\\boldsymbol{A} + \\boldsymbol{B})^T = \\boldsymbol{A}^T + \\boldsymbol{B}^T$$ $$c(\\boldsymbol{A} + \\boldsymbol{B}) = c\\boldsymbol{A} + c\\boldsymbol{B}$$ "},"title":"线性代数"},"/guide/math_mathematical_statistics/":{"data":{"":"","good#good":"test "},"title":"数理统计"},"/guide/math_probability_theory/":{"data":{"a1-概率论原理#A.1 概率论原理":"随机试验是其结果不能提前以确定的方式预测的试验（Ross 1987；Casella 和 Berger 1990）。所有可能的结果的集合称作样本空间 ( S )。一个样本空间是离散的，如果它由结果的有限(或可数无限)集组成；否则是连续的。( S ) 的任意子集 ( E ) 是一个事件。事件是集合，并且我们可以谈论它们的补、交、并等。\n概率的一种解释是频率 (frequency)。当一个试验在完全相同的条件下不断重复时，对于任意事件 ( E )，结果在 ( E ) 中的次数所占的比例趋向于某个常数值。这个常数极限频率是事件的概率，而我们也记作 ( P(E) )。\n有时，概率可解释成可存程度 (degree of belief)。例如，当我们说土耳其赢得 2010 年足球世界杯冠军的概率时，我们并不是指出现的频率，因为 2010 年足球世界杯只进行一次，并且(在写本书时)它还未进行。在这种情况下，我们的意思是我们在观察相位事件中出现的程度。由于是主观的，因此对同一事件，不同的人可能指派不同的概率。\nA.1.1 概率论公理 公理确保随机试验中指派的概率可以解释成相对频率，并且这些指派符合我们对相对频率之间关系的直观理解：\n( 0 \\leq P(E) \\leq 1 )。如果 ( E_1 ) 是不可能出现的事件，则 ( P(E_1) = 0 )。如果 ( E_2 ) 是一定出现的事件，则 ( P(E_2) = 1 )。\n如果 ( S ) 是包含所有可能结果的样本空间，则 ( P(S) = 1 )。\n如果 ( E_i, i = 1, \\cdots, n )，是互斥的(即如果它们不可能同时出现：( E_i \\cap E_j = \\emptyset, i \\neq j )，其中 (\\emptyset) 是不包含任何可能结果的空事件)，则我们有\n[ P\\left(\\bigcup_{i=1}^{n} E_i\\right) = \\sum_{i=1}^{n} P(E_i) ]\n（A.1）\n例如，设 ( E^c ) 表示 ( E ) 的补，由不在 ( E ) 中的 ( S ) 中所有可能的结果组成，我们有 ( E \\cap E^c = \\emptyset )，并且\n[ P(E \\cup E^c) = P(E) + P(E^c) = 1 ]\n[ P(E^c) = 1 - P(E) ]\n如果 ( E ) 和 ( F ) 的交非空，则我们有\n[ P(E \\cup F) = P(E) + P(F) - P(E \\cap F) ]\n（A.2）","概率论#概率论":"概率论我们简略回顾概率论原理、随机变量概念和实例分布。"},"title":"概率论"},"/guide/program_go/":{"data":{"":"","good#good":"test "},"title":"Go"},"/guide/program_java/":{"data":{"":"","good#good":"test "},"title":"Java"},"/guide/program_linux/":{"data":{"":"","-其他命令#🛠️ 其他命令":" 命令 作用 ","-其他实用工具#🛠️ 其他实用工具":" 命令 作用 示例 tar 压缩/解压文件 tar -czvf archive.tar.gz dir/\ntar -xzvf archive.tar.gz crontab 定时任务管理 crontab -e（编辑）\ncrontab -l（查看） history 查看命令历史 history | grep \"apt\" alias 设置命令别名 alias ll='ls -alF' ","-用户与权限管理#👥 用户与权限管理":" 命令 作用 示例 useradd 添加用户 useradd -m newuser passwd 修改密码 passwd username sudo 提权执行命令 sudo apt update chmod 修改权限 chmod 755 script.sh chown 修改文件所有者 chown user:group file.txt ","-系统信息与管理#🖥️ 系统信息与管理":" 命令 作用 示例 top/htop 实时进程监控 top\nhtop（需安装） ps 查看进程 ps aux ps -ef|grep nginx kill 终止进程 kill -9 1234（强制终止 PID 1234） df 查看磁盘空间 df -h（人类可读格式） du 查看目录大小 du -sh /home/（汇总大小） free 查看内存使用 free -h systemctl 管理系统服务 systemctl start nginx\nsystemctl status sshd shutdown 关机/重启 shutdown now\nreboot ","-网络操作#🌐 网络操作":" 命令 作用 示例 ping 测试网络连通性 ping google.com ifconfig/ip 查看/配置网络接口 ifconfig\nip addr curl/wget 下载文件 curl -O http://example.com/file\nwget http://example.com/file ssh 远程登录 ssh user@192.168.1.1 scp 远程复制文件 scp file.txt user@host:/path/ netstat/ss 查看网络连接/端口 netstat -tulnp\nss -tulnp ","-软件包管理#📦 软件包管理":" 命令（Debian/Ubuntu） 命令（RHEL/CentOS） 作用 示例 apt update yum update 更新软件包列表 sudo apt update apt install yum install 安装软件 sudo apt install nginx apt remove yum remove 卸载软件 sudo apt remove nginx dpkg -i rpm -i 安装本地包 sudo dpkg -i package.deb ","1-linux-概述#1. Linux 概述":"1.1. Linux 是什么？ Linux 是一个开源、免费的操作系统内核，由芬兰程序员 Linus Torvalds 在 1991 年 首次发布。 广泛应用于服务器、超级计算机、嵌入式设备（如路由器、智能电视）以及个人电脑（如 Ubuntu、Fedora）。 1.2. Linux 系统目录结构 /\t[根目录，所有目录的起点] ├── bin [核心命令：存放系统启动和运行必需的基础命令，如 ls/cp/bash] ├── sbin [管理员命令：存放系统管理命令，如 iptables/reboot，需 root 权限] ├── boot [启动文件：包含内核(vmlinuz)和引导加载器(GRUB)文件] ├── dev [设备文件：硬件设备抽象文件，如 /dev/sda(硬盘)、/dev/tty(终端)] ├── etc [配置文件：系统和应用程序的配置，如 /etc/passwd(用户)、/etc/nginx/] ├── home [用户目录：普通用户的家目录，如 /home/username 存放个人文件] ├── root [root家目录：超级用户的家目录，普通用户无权访问] ├── lib [核心库：存放/bin和/sbin所需的共享库(.so文件)] ├── usr [用户软件：相当于Windows的Program Files] │ ├── bin [非必需的用户命令] │ ├── sbin [非必需的管理命令] │ └── lib [应用程序库文件] ├── opt [第三方软件：商业软件或大型应用，如 Oracle/Matlab] ├── proc [内核/进程信息：虚拟文件系统，实时反映进程和内核状态] ├── sys [系统设备：虚拟文件系统，管理设备驱动和内核参数] 数据：存放系统启动后的临时文件(如PID文件)] ├── var [可变数据：经常变化的文件] │ ├── log [系统日志：如 /var/log/syslog] │ └── cache [应用程序缓存] ├── tmp [临时文件：所有用户可读写，重启后通常清空] ├── mnt [手动挂载：临时挂载点(如U盘/硬盘)] └── media [自动挂载：可移动设备自动挂载点(如光盘/U盘)] 1.3. Linux 文件基本属性: Tip\nLinux 系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。\n文件类型 所有者权限 组权限 其他用户权限 ↓ ↓ ↓ ↓ - r w x r - - r - - ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ │ │ └执行 │ │ └执行 │ │ └执行 │ └─写入 │ └─写入 │ └─写入 └──读取 └──读取 └──读取 1️⃣ 文件类型：\nd（文件\\目录）； -（文本、二进制、压缩包等常规文件）； l（符号链接，指向另一个文件的快捷方式（软链接）） …… 2️⃣ 用户身份划分：\n所有者（Owner）：文件的创建者/拥有者，拥有最高控制权。 所属组（Group）：文件所属的用户组，组内成员共享组权限。 其他用户（Others）：既不是所有者也不在所属组的其他用户。 3️⃣ 权限类型：每种用户身份对应三种基本权限：\n读 r：查看文件内容或列出目录内容。 写 w：修改文件内容或在目录中创建/删除文件。 执行 x：运行文件（如脚本）或进入目录。 4️⃣ 权限表示法：符号模式；数字模式\n符号模式： 用 r，w，x 表示权限； 数字模式： 用八进制数表示，每位数字对应 r=4、w=2、x=1 的和； 符号模式 数字模式 权限案例 rw-r--r-- 644 所有者 rw-（读写） 6 = 4+2（读写） 所属组 r--（只读） 4 = 4（只读） 其他用户 r--（只读） 4 = 4（只读） 1.4. Linux 常用命令 📂 文件\\文件夹权限 ls, chmod, chown, chgrp 📂 文件\\文件夹权限： 1️⃣ 使用 ls -l 命令查看文件权限：\n$ ls -l -rw-r--r-- 1 alice dev 1024 Sep 1 10:00 file.txt 第1字段 -rw-r--r--：文件类型和权限。 第3字段 alice：所有者。 第4字段 dev：所属组。 2️⃣ chmod 更改权限\n$ chmod u+x script.sh $ chmod g-w file.txt $ chmod 755 script.sh 给所有者添加执行权限 移除所属组的写权限 数字模式：rwxr-xr-x 3️⃣ chown 更改所有者和所属组\n$ chown alice:dev file.txt 修改所有者为 alice，组为 dev 4️⃣ chgrp 单独修改所属组\n$ chgrp dev file.txt 🛣 路径查看\\切换： ：\n👻\n：\n文件与目录操作 命令 作用\\实例 xx --help 查看帮助： pwd 1️⃣ $ cd /home（切换目录）\n$ cd ..（返回上级） ls 1️⃣ $ ls -l（列出目录内容）\n2️⃣ $ ls -a（显示隐藏文件） chmod 1️⃣ $ chmod u+x test.sh（）\n2️⃣ $ chmod g-w file.txt（）\n$ chmod 755 test.sh（） touch 创建空文件或更新时间戳\n$ touch newfile.txt https://www.unicode.org/emoji/charts/full-emoji-list.html\n命令 作用 示例 mkdir 创建目录 mkdir dir\nmkdir -p dir1/dir2（递归创建） rm 删除文件/目录 rm file.txt\nrm -r dir/（递归删除）\nrm -f file 或 dir/（强制删除） cp 复制文件/目录 cp file1.txt file2.txt\ncp -r dir1/ dir2/ mv 移动/重命名文件 mv old.txt new.txt\nmv file /tmp/ cat 查看文件内容 cat file.txt grep 文本搜索 grep \"error\" log.txt\ngrep -r \"pattern\" /dir/ find 查找文件 find /home -name \"*.txt\" ","good#good":"test ","参考网址#参考网址：":" https://www.w3ccoo.com/linux/ https://www.tutorialspoint.com/unix/unix-getting-started.htm "},"title":"Linux\\Shell"},"/guide/program_python/":{"data":{"":"","good#good":"test "},"title":"Python"},"/guide/program_scala/":{"data":{"":"","good#good":"test "},"title":"Scala"},"/guide/python_flask/":{"data":{"":"","good#good":"test "},"title":"Python_Flask"},"/guide/python_folium/":{"data":{"":"","good#good":"test "},"title":"Python_Folium"},"/guide/python_geopandas/":{"data":{"":"","good#good":"test "},"title":"Python_GeoPandas"},"/guide/python_matplotlib/":{"data":{"":"","0-速查表#0. 速查表":" 对象 设置 方法 OO-style pyplot-style 创建 Figure _ fig = plt.figure(figsize=(3,3)) plt.figure(figsize=(3,3)) 创建 Axes _ fig, ax = plt.subplots(figsize=(3,3)) plt.plot() 标题 内容 _ ax.set_title('Title') plt.title('Title') 标题 格式 参数 ax.set_title('Title', loc='left', font='Georgia', size=12, color='gray', weight='bold') plt.title('Title', loc='left', font='Georgia', size=12, color='gray', weight='bold') 标题 格式 参数字典 format_dir = {'fontproperties': 'Georgia', 'fontsize': 15, 'color': 'Gray', 'fontweight': 'bold'}\nax.set_title('Title', loc='left', **format_dir) format_dir = {'fontproperties': 'Georgia', 'fontsize': 15, 'color': 'Gray', 'fontweight': 'bold'}\nplt.title('Title', loc='left', **format_dir) 标题 位置 简单设置 ax.set_title('Title', loc='left') \u003cloc\u003e: ’left’,‘center’,‘right’ plt.title(\"Title\", loc='left') \u003cloc\u003e: ’left’,‘center’,‘right’ 设置类容 OO-style pyplot-style 关闭 坐标轴 ax.axis('off')# 关闭所有坐标轴 ax.spines['top'].set_visible(False) plt.axis('off') # 关闭所有坐标轴 plt.gca().spines['top'].set_visible(False) 坐标轴 颜色 ax..spines['bottom'].set_color('red') plt.gca().spines['bottom'].set_color('red') 坐标轴 粗细 ax.spines['left'].set_linewidth(0.5) plt.gca().spines['left'].set_linewidth(0.5) 坐标轴 位置 1\n单个参数 ax.spines[\"right\"].set_position()\n# `` 坐标轴 位置 1 ax.spines[\"right\"].set_position((\"outward\", 5))\n# “inward”：向内，“outward”：向外 plt.gca().spines[\"right\"].set_position((\"outward\", 5)) # “inward”：向内，“outward”：向外 `` `` ","1-概述#1. 概述":"1.1. 编码风格 (1). the OO-style import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2, 100) # Sample data. fig, ax = plt.subplots(figsize=(3.5, 3), layout='constrained') # Create a figure ax.plot(x, x, label='linear') # Plot some data on the Axes. ax.plot(x, x**2, label='quadratic') # Plot more data on the Axes... ax.set_title(\"Simple Plot\") # Add a title to the Axes. ax.set_xlabel('x label') # Add an x-label to the Axes. ax.set_ylabel('y label') # Add a y-label to the Axes. ax.legend() # Add a legend. plt.show() # Show the figure. (2). pyplot-style import numpy as np import matplotlib.pyplot as plt x = np.linspace(0, 2, 100) # Sample data. plt.figure(figsize=(3.5, 3), layout='constrained') plt.plot(x, x, label='linear') # Plot some data on the (implicit) Axes. plt.plot(x, x**2, label='quadratic') # etc. plt.title(\"Simple Plot\") plt.xlabel('x label') plt.ylabel('y label') plt.legend() # Add a legend. plt.show() # Show the figure. 1.2. 图形结构 (1). Figure 📝 Figure 是整个窗体，你可以把它想象成一个画板，我们在其上面创建图形进，这是最为外层的对象。 Figure 可以将其看作是整个绘图区域的容器 一个 Figure 对象可以包含一或多个 Axes （子图）对象。 OO-stylepyplot-style import matplotlib.pyplot as plt # an empty figure with no Axes fig = plt.figure(figsize=(3.5, 3), layout='constrained') plt.show() 输出结果如下：\n\u003cFigure size 350x300 with 0 Axes\u003e import matplotlib.pyplot as plt # an empty figure with no Axes plt.figure(figsize=(3.5, 3), layout='constrained') plt.show() 输出结果如下：\n\u003cFigure size 350x300 with 0 Axes\u003e (2). Axes 📝 Axes （子图）对象，是实际进行绘图操作的区域，包含坐标轴、数据点、线条、图例等元素。 Axes 可以将其看作是包含绘图元素（例如线、点、刻度、标签、图例等）的一个容器。 Axes 指子图，每个子图可以用任一种坐标系表示。如笛卡尔坐标系（直角坐标系），它包含两个 Axis 对象。每个 Axes 都有一个标题，一个 x 标签和一个 y 标签。在这个坐标系内，我们可以绘制各种图形。 OO-stylepyplot-style import matplotlib.pyplot as plt # Create a figure with a single Axes fig, ax = plt.subplots(figsize=(3.5, 3)) plt.show() 输出结果如下：\nimport matplotlib.pyplot as plt # Create a figure plt.figure(figsize=(3.5, 3)) # Create a single Axes plt.plot() plt.show() 输出结果如下：\n(3). Axis 📝 Axis （轴）对象，用于控制图形中的刻度、刻度标签和网格线。 每个 Axes 对象有一个 XAxis 对象和一个 YAxis 对象。 Axis 的主要功能包括：设置标签格式；刻度位置；刻度标签、字体、颜色；刻度线粗细、颜色；网格线显示与否、样式等； OO-stylepyplot-style import matplotlib.pyplot as plt # Create a figure containing a single Axes. fig, ax = plt.subplots(figsize=(3.5, 3)) # Plot some data on the Axes. ax.plot([1, 2, 3, 4], [1, 4, 2, 3]) # Create label formatting dictionary label_format = {'fontproperties': 'Georgia', 'fontsize': 10, 'color': 'Gray', 'fontweight': 'bold'} # Set title ax.set_title('This is Title', **label_format, size=15) # Set labels for the x-axis and y-axis ax.set_xlabel('X Axis Title Here', **label_format) ax.set_ylabel('Y Axis Title Here', **label_format) # Show the figure plt.show() 输出结果如下：\nimport matplotlib.pyplot as plt plt.figure(figsize=(3.5, 3)) plt.plot([1, 2, 3, 4], [1, 4, 2, 3]) # Create label formatting dictionary label_format = {'fontproperties': 'Georgia', 'fontsize': 10, 'color': 'Gray', 'fontweight': 'bold'} plt.title('This is Title', **label_format, size=15) plt.xlabel('X Axis Title Here', **label_format) plt.ylabel('Y Axis Title Here', **label_format) plt.show() 输出结果如下：","2-颜色#2. 颜色":"","3-文本#3. 文本":"","4-figure-图形设置#4. Figure 图形设置":"","5-axes-子图细节设置#5. Axes 子图细节设置":"5.1. 标题： 1️⃣ OO-style 位置 2️⃣ OO-style 格式参数 3️⃣ OO-style 格式字典 4️⃣ pyplot-style 位置 5️⃣ pyplot-style 格式参数 6️⃣ pyplot-style 格式字典 import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(3,3)) ax.plot([1, 2, 3, 4], [8, 4, 2, 3]) ax.set_title('Title', loc='left') plt.show() import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(3,3)) ax.plot([1, 2, 3, 4], [8, 4, 2, 3]) # Set title ax.set_title('This is Title', loc='left', font='Georgia', size=12, color='gray', weight='bold') plt.show() import matplotlib.pyplot as plt fig, ax = plt.subplots(figsize=(3,3)) ax.plot([1, 2, 3, 4], [8, 4, 2, 3]) # Create label formatting dictionary format_dir = {'fontproperties': 'Georgia', 'fontsize': 15, 'color': 'Gray', 'fontweight': 'bold'} # Set title ax.set_title('This is Title', loc='left', **format_dir) plt.show() import matplotlib.pyplot as plt # Create a figure plt.figure(figsize=(3, 3)) # Create a single Axes plt.plot([1, 2, 3, 4], [8, 4, 2, 3]) plt.title(\"Title\", loc='left') # 将标题设置在左侧 plt.show() import matplotlib.pyplot as plt plt.figure(figsize=(3,3)) plt.plot([1, 2, 3, 4], [8, 4, 2, 3]) # Set title plt.title('Title', loc='left', font='Georgia', size=12, color='gray', weight='bold') plt.show() import matplotlib.pyplot as plt plt.figure(figsize=(3, 3)) plt.plot([1, 2, 3, 4], [8, 4, 2, 3]) # Create label formatting dictionary format_dir = {'fontproperties': 'Georgia', 'fontsize': 15, 'color': 'Gray', 'fontweight': 'bold'} # Set title plt.title('Title', loc='left', **format_dir) plt.show() 5.2. 坐标轴： OO-stylepyplot-style 输出结果如下：\n输出结果如下：","6-多子图绘制#6. 多子图绘制":"6.1. "},"title":"Python_Matplotlib"},"/guide/python_numpy/":{"data":{"":"","good#good":"test "},"title":"Python_Numpy"},"/guide/python_pandas/":{"data":{"":"","good#good":"test "},"title":"Python_Pandas"},"/guide/python_plotly/":{"data":{"":"","good#good":"test "},"title":"Python_Plotly"},"/guide/python_seaborn/":{"data":{"":"","good#good":"test "},"title":"Python_Seaborn"},"/guide/python_shap/":{"data":{"":"","good#good":"test "},"title":"Python_SHAP"},"/guide/python_sklearn/":{"data":{"":"","good#good":"test "},"title":"Python_Sklearn"},"/guide/python_sklearn_source_code/":{"data":{"":"","good#good":"test "},"title":"Python_Sklearn 源码"},"/guide/python_statsmodels/":{"data":{"":"","good#good":"test "},"title":"Python_Statsmodels"},"/guide/python_streamlit/":{"data":{"":"","good#good":"test "},"title":"Python_Streamlit"}}